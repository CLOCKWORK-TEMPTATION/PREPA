import os
import re
import json
import time
import logging
from dataclasses import dataclass, field, asdict
from typing import Any, Optional, List
from pathlib import Path
from collections import defaultdict

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('screenplay_dataset.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# --- Fail-Safe Imports ---
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    pd = None

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    nx = None

try:
    from sentence_transformers import SentenceTransformer
    from transformers import pipeline as hf_pipeline
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    SentenceTransformer = None
    hf_pipeline = None

try:
    from google import genai
    from google.genai import types as genai_types
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    genai = None
    genai_types = None
    print("WARNING: Ù…ÙƒØªØ¨Ø© google-genai ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. pip install google-genai")

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ­Ø¯Ø© ØªÙˆØ­ÙŠØ¯ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
try:
    from entity_canonicalizer import EntityCanonicalizer, canonicalize_scenes, SIMILARITY_AVAILABLE
    CANONICALIZER_AVAILABLE = True
except ImportError:
    CANONICALIZER_AVAILABLE = False
    SIMILARITY_AVAILABLE = False
    print("WARNING: ÙˆØ­Ø¯Ø© entity_canonicalizer ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")

# Docling Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF
try:
    from docling.document_converter import DocumentConverter, PdfFormatOption
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import (
        PdfPipelineOptions,
        TableFormerMode,
        EasyOcrOptions,
    )
    from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions
    DOCLING_AVAILABLE = True
except ImportError:
    DOCLING_AVAILABLE = False
    print("WARNING: Ù…ÙƒØªØ¨Ø© Docling ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. pip install docling")

# Ù‚Ø±Ø§Ø¡Ø© API Keys Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
UNSTRUCTURED_API_KEY = os.getenv("UNSTRUCTURED_API_KEY", "")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")

# ---------------------------------------------------------
# Configuration
# ---------------------------------------------------------
class Config:
    CONTEXT_WINDOW_SIZE = 5
    MIN_DIALOGUE_LENGTH = 2
    EMBEDDING_MODEL = 'intfloat/multilingual-e5-small'
    SENTIMENT_MODEL = 'CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment'
    GEMINI_MODEL = 'gemini-3-flash-preview'  # New SDK format
    
    SCENE_PATTERN = re.compile(r"^\s*(?:Ù…Ø´Ù‡Ø¯|Ù…\.|Scene)\s*[:\-]?\s*(\d+)|^(?:Ø¯Ø§Ø®Ù„ÙŠ|Ø®Ø§Ø±Ø¬ÙŠ|INT\.|EXT\.)", re.IGNORECASE)
    SPEAKER_PATTERN = re.compile(r"^\s*([Ø£-ÙŠa-zA-Z\s]{2,25})\s*(?::)?\s*$")
    TRANSITIONS = {"Ù‚Ø·Ø¹", "ÙƒØ§Øª", "CUT", "FADE OUT", "FADE IN", "Ø¥Ø¸Ù„Ø§Ù…", "ØªÙ„Ø§Ø´ÙŠ"}

# ---------------------------------------------------------
# Data Models
# ---------------------------------------------------------
@dataclass
class DialogueTurn:
    scene_id: str
    turn_id: int
    speaker: str
    text: str
    normalized_text: str = ""
    sentiment: str = "unknown"
    sentiment_score: float = 0.0

@dataclass
class Scene:
    scene_id: str
    scene_number: int
    heading: str
    location: str
    time_of_day: str
    int_ext: str
    actions: List[str] = field(default_factory=list)
    dialogue: List[DialogueTurn] = field(default_factory=list)
    characters: List[str] = field(default_factory=list)
    full_text: str = ""
    embedding: Optional[List[float]] = None

# ---------------------------------------------------------
# Arabic Text Utilities
# ---------------------------------------------------------
def normalize_arabic(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'[\u064B-\u065F\u0670]', '', text)  # Remove diacritics
    text = re.sub(r'\u0640+', '', text)  # Remove tatweel
    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
    text = re.sub(r'[ÙŠÙ‰]', 'ÙŠ', text)
    text = re.sub(r'Ø©', 'Ù‡', text)
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
    return text.strip()

def count_arabic_words(text: str) -> int:
    if not text:
        return 0
    return len(re.findall(r'[\u0600-\u06FF]+', text))

# ---------------------------------------------------------
# File Ingestion
# ---------------------------------------------------------
class TextFileIngestor:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª TXT Ù…Ø¨Ø§Ø´Ø±Ø©"""
    def process(self, file_path: str) -> List[str]:
        logger.info(f"Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†ØµÙŠ: {file_path}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.readlines()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='utf-8-sig') as f:
                return f.readlines()
        except Exception as e:
            logger.error(f"ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")
            return []

class DoclingIngestor:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª PDF Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Docling Ù…Ø¹ OCR Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    def __init__(self):
        if not DOCLING_AVAILABLE:
            raise RuntimeError("Ù…ÙƒØªØ¨Ø© Docling ØºÙŠØ± Ù…Ø«Ø¨ØªØ©!")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª OCR Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True
        pipeline_options.ocr_options = EasyOcrOptions(lang=["ar", "en"])
        pipeline_options.do_table_structure = False
        pipeline_options.accelerator_options = AcceleratorOptions(
            num_threads=4, device=AcceleratorDevice.AUTO
        )
        
        self.converter = DocumentConverter(
            format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}
        )

    def process(self, file_path: str) -> List[str]:
        logger.info(f"Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù PDF Ø¨ØµØ±ÙŠØ§Ù‹: {file_path}")
        try:
            result = self.converter.convert(file_path)
            md_text = result.document.export_to_markdown()
            return md_text.split('\n')
        except Exception as e:
            logger.error(f"ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† PDF: {e}")
            return []

def get_ingestor(file_path: str):
    """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù"""
    ext = Path(file_path).suffix.lower()
    if ext in ['.txt', '.md']:
        return TextFileIngestor()
    elif ext == '.pdf':
        if not DOCLING_AVAILABLE:
            raise RuntimeError("Ù…ÙƒØªØ¨Ø© Docling ØºÙŠØ± Ù…Ø«Ø¨ØªØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© PDF! pip install docling")
        return DoclingIngestor()
    else:
        return TextFileIngestor()

# ---------------------------------------------------------
# Screenplay Parser
# ---------------------------------------------------------
class ScreenplayParser:
    def __init__(self):
        self.normalizer = self
    
    def normalize(self, text: str) -> str:
        return normalize_arabic(text)
    
    def _clean_markdown(self, line: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø·Ø± Ù…Ù† Ø¹Ù„Ø§Ù…Ø§Øª Markdown"""
        return line.replace('**', '').replace('###', '').replace('##', '').replace('#', '').strip()
    
    def parse(self, lines: List[str]) -> List[Scene]:
        scenes: List[Scene] = []
        current_scene: Optional[Scene] = None
        current_speaker: Optional[str] = None
        current_turn_lines: List[str] = []
        turn_counter = 0

        def flush_turn():
            nonlocal current_speaker, current_turn_lines, turn_counter
            if current_scene and current_speaker and current_turn_lines:
                full_text = " ".join(current_turn_lines).strip()
                if full_text:
                    turn_counter += 1
                    # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ§Ù„Ù…Ø·Ø¨Ø¹
                    norm_text = self.normalizer.normalize(full_text)
                    current_scene.dialogue.append(DialogueTurn(
                        scene_id=current_scene.scene_id,
                        turn_id=turn_counter,
                        speaker=current_speaker,
                        text=full_text,
                        normalized_text=norm_text
                    ))
                    if current_speaker not in current_scene.characters:
                        current_scene.characters.append(current_speaker)
            current_speaker = None
            current_turn_lines = []

        def finalize_scene(scene: Scene):
            parts = [scene.heading] + scene.actions
            for dt in scene.dialogue:
                parts.append(f"{dt.speaker}: {dt.text}")
            scene.full_text = "\n".join(parts)

        # Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        for raw_line in lines:
            line = self._clean_markdown(raw_line)
            if not line: continue

            # 1. Ø§ÙƒØªØ´Ø§Ù Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ø´Ù‡Ø¯
            scene_match = Config.SCENE_PATTERN.search(line)
            # Ø´Ø±Ø· Ø¥Ø¶Ø§ÙÙŠ: Ø§Ù„Ø³Ø·Ø± Ù„ÙŠØ³ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ Ù„ÙŠÙƒÙˆÙ† ÙˆØµÙØ§Ù‹
            is_header = scene_match and len(line) < 60

            if is_header:
                if current_scene:
                    flush_turn()
                    finalize_scene(current_scene)
                    scenes.append(current_scene)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ù‚Ù… Ø¥Ù† ÙˆØ¬Ø¯
                num_match = re.search(r'\d+', line)
                num = int(num_match.group(0)) if num_match else len(scenes) + 1
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§
                time_val = next((t for t in ["Ù„ÙŠÙ„", "Ù†Ù‡Ø§Ø±", "Ù…Ø³Ø§Ø¡", "ØµØ¨Ø§Ø­"] if t in line), "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
                loc_val = re.sub(r'(Ù…Ø´Ù‡Ø¯|Ù…\.|Scene|\d+|Ù„ÙŠÙ„|Ù†Ù‡Ø§Ø±|Ø®Ø§Ø±Ø¬ÙŠ|Ø¯Ø§Ø®Ù„ÙŠ|[\-\.])', '', line).strip()
                
                current_scene = Scene(
                    scene_id=f"S{num:04d}",
                    scene_number=num,
                    heading=line,
                    location=loc_val or "Ù…ÙˆÙ‚Ø¹ ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                    time_of_day=time_val,
                    int_ext="Ø¯Ø§Ø®Ù„ÙŠ" if "Ø¯Ø§Ø®Ù„ÙŠ" in line else "Ø®Ø§Ø±Ø¬ÙŠ"
                )
                turn_counter = 0
                continue

            if current_scene is None: continue

            # 2. Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„Ø§Øª (Transitions)
            if line in Config.TRANSITIONS:
                flush_turn()
                current_scene.actions.append(f"[TRANSITION: {line}]")
                continue

            # 3. Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ØªØ­Ø¯Ø« (Heuristics + Regex)
            speaker_match = Config.SPEAKER_PATTERN.match(line)
            # Ø§Ù„Ø´Ø±ÙˆØ·: ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„Ù†Ù…Ø· + Ù„ÙŠØ³ Ø·ÙˆÙŠÙ„Ø§Ù‹ + Ù„ÙŠØ³ ÙØ¹Ù„Ø§Ù‹ Ø¨ÙŠÙ† Ù‚ÙˆØ³ÙŠÙ†
            if speaker_match and len(line.split()) <= 4 and not line.startswith('('):
                potential_name = speaker_match.group(1).strip()
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØ´Ø¨Ù‡ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
                if potential_name not in ["ØµÙˆØª", "ØªÙƒÙ…Ù„Ø©", "ØªØ§Ø¨Ø¹"]:
                    flush_turn()
                    current_speaker = potential_name
                    continue

            # 4. Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø­ÙˆØ§Ø± Ø£Ùˆ Ø§Ù„ÙˆØµÙ
            if current_speaker:
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø·Ø± Ø¨ÙŠÙ† Ù‚ÙˆØ³ÙŠÙ†ØŒ Ù†Ø¹ØªØ¨Ø±Ù‡ "Parenthetical" (ÙˆØµÙ Ø·Ø±ÙŠÙ‚Ø© Ù†Ø·Ù‚) ÙˆÙ„Ø§ Ù†Ø¶ÙŠÙÙ‡ Ù„Ù„Ø­ÙˆØ§Ø± Ø§Ù„ØµØ§ÙÙŠ
                if line.startswith('(') and line.endswith(')'):
                    continue 
                current_turn_lines.append(line)
            else:
                current_scene.actions.append(line)

        # Ø¥Ø¶Ø§ÙØ© Ø¢Ø®Ø± Ù…Ø´Ù‡Ø¯
        if current_scene:
            flush_turn()
            finalize_scene(current_scene)
            scenes.append(current_scene)

        return scenes

# ---------------------------------------------------------
# 6. Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ø°ÙƒÙŠ (Enrichment Layer)
# ---------------------------------------------------------
class AIEnricher:
    """
    Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ø´Ø§Ù‡Ø¯ ÙˆØ§Ù„Ø­ÙˆØ§Ø±Ø§Øª

    ØªØ´Ù…Ù„:
    - ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª (Entity Canonicalization)
    - ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings)
    - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (Sentiment Analysis)
    """

    def __init__(self, use_gpu=True, similarity_threshold: float = 0.85):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡

        Args:
            use_gpu: Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
            similarity_threshold: Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 85%)
        """
        self.embedder = None
        self.sentiment_analyzer = None
        self.canonicalizer = None
        self.canonicalization_stats = {}

        # ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆØ­Ø¯ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
        if CANONICALIZER_AVAILABLE and SIMILARITY_AVAILABLE:
            try:
                self.canonicalizer = EntityCanonicalizer(similarity_threshold=similarity_threshold)
                logger.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆØ­Ø¯ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª (Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {similarity_threshold:.0%})")
            except Exception as e:
                logger.warning(f"ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆØ­Ø¯ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª: {e}")
        else:
            logger.warning("ÙˆØ­Ø¯Ø© ØªÙˆØ­ÙŠØ¯ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© - ØªØ®Ø·ÙŠ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡")

        if ML_AVAILABLE:
            try:
                logger.info("ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings (E5-Small)...")
                self.embedder = SentenceTransformer(Config.EMBEDDING_MODEL, device='cuda' if use_gpu else 'cpu')

                logger.info("ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (CamelBERT)...")
                self.sentiment_analyzer = hf_pipeline("text-classification", model=Config.SENTIMENT_MODEL, device=0 if use_gpu else -1)
            except Exception as e:
                logger.warning(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")

    def canonicalize_entities(self, scenes: List[Scene], merge_log_path: Optional[Path] = None) -> List[Scene]:
        """
        ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª ÙÙŠ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯

        ÙŠÙ‚ÙˆÙ… Ø¨Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØªØ·Ø¨ÙŠÙ‚Ù‡ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª

        Args:
            scenes: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯
            merge_log_path: Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ø¯Ù…Ø¬ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)

        Returns:
            Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ Ø¨Ø¹Ø¯ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
        """
        if not self.canonicalizer:
            logger.info("Ù…ÙˆØ­Ø¯ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ± - ØªØ®Ø·ÙŠ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡")
            return scenes

        logger.info("Ø¨Ø¯Ø¡ ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª...")

        # Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        canonical_map = self.canonicalizer.build_canonical_map(scenes)

        if canonical_map:
            logger.info(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(canonical_map)} Ø§Ø³Ù… Ù„Ù„ØªÙˆØ­ÙŠØ¯")

            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØ­ÙŠØ¯
            scenes = self.canonicalizer.apply_normalization(scenes)

            # Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ø¯Ù…Ø¬
            if merge_log_path:
                self.canonicalizer.export_merge_log(merge_log_path)

            # Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.canonicalization_stats = self.canonicalizer.get_statistics()
            logger.info(f"Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙˆØ­ÙŠØ¯: {self.canonicalization_stats}")
        else:
            logger.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø³Ù…Ø§Ø¡ Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù„Ù„ØªÙˆØ­ÙŠØ¯")

        return scenes

    def enrich(self, scenes: List[Scene], canonicalize: bool = True, merge_log_path: Optional[Path] = None) -> List[Scene]:
        """
        Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª

        ÙŠØ´Ù…Ù„: ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ØŒ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§ØªØŒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±

        Args:
            scenes: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯
            canonicalize: ØªØ·Ø¨ÙŠÙ‚ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: True)
            merge_log_path: Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ø³Ø¬Ù„ Ø¯Ù…Ø¬ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡

        Returns:
            Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡
        """
        # 1. ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª (Ø£ÙˆÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø£ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø±Ù‰)
        if canonicalize:
            scenes = self.canonicalize_entities(scenes, merge_log_path)

        # 2. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
        if self.embedder:
            logger.info("Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings)...")
            texts = [f"passage: {s.full_text[:2000]}" for s in scenes]
            embeddings = self.embedder.encode(texts, show_progress_bar=True, batch_size=16)
            for i, scene in enumerate(scenes):
                scene.embedding = embeddings[i].tolist()

        # 3. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        if self.sentiment_analyzer:
            logger.info("Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª...")
            for scene in scenes:
                for turn in scene.dialogue:
                    try:
                        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙÙ‚Ø· Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
                        res = self.sentiment_analyzer(turn.text[:400])[0]
                        turn.sentiment = res['label']
                        turn.sentiment_score = res['score']
                    except:
                        pass

        return scenes

    def get_canonicalization_stats(self) -> dict:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡

        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        """
        return self.canonicalization_stats

    def build_social_graph(self, scenes: List[Scene]):
        """Build character interaction graph"""
        if not NETWORKX_AVAILABLE:
            logger.warning("Ù…ÙƒØªØ¨Ø© networkx ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© - ØªØ®Ø·ÙŠ Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª")
            return None
        G = nx.Graph()
        import itertools
        for scene in scenes:
            chars = list(set(scene.characters))
            if len(chars) < 2: continue
            for c1, c2 in itertools.combinations(chars, 2):
                if G.has_edge(c1, c2):
                    G[c1][c2]['weight'] += 1
                else:
                    G.add_edge(c1, c2, weight=1)
        return G

# ---------------------------------------------------------
# 7. Ù…Ø­Ù„Ù„ Gemini Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (Advanced AI Analysis)
# ---------------------------------------------------------
class GeminiAnalyzer:
    def __init__(self):
        self.client = None
        if GEMINI_AVAILABLE and GEMINI_API_KEY:
            try:
                # New google-genai SDK (December 2025+)
                self.client = genai.Client(api_key=GEMINI_API_KEY)
                logger.info(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Gemini ({Config.GEMINI_MODEL}) Ø¨Ù†Ø¬Ø§Ø­")
            except Exception as e:
                logger.error(f"ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Gemini: {e}")
        else:
            logger.warning("Gemini ØºÙŠØ± Ù…ØªÙˆÙØ± - ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ API Key")
    
    def _call_gemini(self, prompt: str) -> str:
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¢Ù…Ù† Ù„Ù€ Gemini Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        if not self.client:
            return "Gemini ØºÙŠØ± Ù…ØªÙˆÙØ±"
        try:
            response = self.client.models.generate_content(
                model=Config.GEMINI_MODEL,
                contents=prompt
            )
            return response.text
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Gemini: {e}")
            return f"Ø®Ø·Ø£: {e}"
    
    def analyze_sentiment_deep(self, scenes: List[Scene]) -> dict:
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø´Ø§Ø¹Ø± Ø¹Ø¨Ø± Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ"""
        if not self.client:
            return {}
        
        logger.info("ğŸ­ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨ÙˆØ§Ø³Ø·Ø© Gemini...")
        
        # ØªØ¬Ù…ÙŠØ¹ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª
        sample_dialogues = []
        for scene in scenes[:10]:  # Ø£ÙˆÙ„ 10 Ù…Ø´Ø§Ù‡Ø¯
            for turn in scene.dialogue[:3]:  # Ø£ÙˆÙ„ 3 Ø­ÙˆØ§Ø±Ø§Øª Ù„ÙƒÙ„ Ù…Ø´Ù‡Ø¯
                sample_dialogues.append({
                    "Ù…Ø´Ù‡Ø¯": scene.scene_number,
                    "Ù…ØªØ­Ø¯Ø«": turn.speaker,
                    "Ù†Øµ": turn.text[:200]  # Ø£ÙˆÙ„ 200 Ø­Ø±Ù
                })
        
        prompt = f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù†ÙØ³ÙŠ ÙˆÙ…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ©.

Ø­Ù„Ù„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª:

{json.dumps(sample_dialogues, ensure_ascii=False, indent=2)}

Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ ÙŠØ´Ù…Ù„:
1. Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ
2. ØªØ·ÙˆØ± Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¹Ø¨Ø± Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯
3. Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø¹Ø§Ø·ÙÙŠØ©
4. Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON:
{{
    "Ø§Ù„Ø­Ø§Ù„Ø©_Ø§Ù„Ø¹Ø§Ù…Ø©": "...",
    "ØªØ·ÙˆØ±_Ø§Ù„Ù…Ø´Ø§Ø¹Ø±": [...],
    "Ø§Ù„Ø´Ø®ØµÙŠØ§Øª_Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©": [...],
    "Ø§Ù„Ø£Ù†Ù…Ø§Ø·_Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©": [...]
}}"""
        
        result = self._call_gemini(prompt)
        try:
            json_match = re.search(r'\{[\s\S]*\}', result)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        return {"raw_analysis": result}
    
    def analyze_character_development(self, scenes: List[Scene]) -> dict:
        """ØªØ­Ù„ÙŠÙ„ ØªØ·ÙˆØ± Ø§Ù„Ø´Ø®ØµÙŠØ§Øª"""
        if not self.client:
            return {}
        
        logger.info("ğŸ‘¥ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ ØªØ·ÙˆØ± Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø¨ÙˆØ§Ø³Ø·Ø© Gemini...")
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        char_counts = defaultdict(int)
        for scene in scenes:
            for turn in scene.dialogue:
                char_counts[turn.speaker] += 1
        
        main_chars = [char for char, count in sorted(char_counts.items(), key=lambda x: x[1], reverse=True)[:5]]
        
        analyses = {}
        for char in main_chars:
            # Ø¬Ù…Ø¹ Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„ÙˆØ³Ø· ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ©
            char_dialogues = []
            for scene in scenes:
                for turn in scene.dialogue:
                    if turn.speaker == char:
                        char_dialogues.append({
                            "Ù…Ø´Ù‡Ø¯": scene.scene_number,
                            "Ù†Øµ": turn.text[:150]
                        })
            
            if len(char_dialogues) < 3: continue
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø¥Ù„Ù‰ Ø«Ù„Ø§Ø« Ù…Ø±Ø§Ø­Ù„
            third = len(char_dialogues) // 3
            start = char_dialogues[:third]
            middle = char_dialogues[third:2*third]
            end = char_dialogues[2*third:]
            
            prompt = f"""Ø£Ù†Øª Ù†Ø§Ù‚Ø¯ Ø£Ø¯Ø¨ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ©.

Ø­Ù„Ù„ ØªØ·ÙˆØ± Ø´Ø®ØµÙŠØ© "{char}" Ø¹Ø¨Ø± Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ:

Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©:
{json.dumps(start, ensure_ascii=False, indent=2)}

Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„ÙˆØ³Ø·:
{json.dumps(middle, ensure_ascii=False, indent=2)}

Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ù†Ù‡Ø§ÙŠØ©:
{json.dumps(end, ensure_ascii=False, indent=2)}

Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ ÙŠØ´Ù…Ù„:
1. Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
2. Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ­ÙˆÙ„ Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ©
3. Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¹Ø§Ø·ÙÙŠ
4. Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
5. Ø§Ù„Ù‚ÙˆØ³ Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠ Ù„Ù„Ø´Ø®ØµÙŠØ© (Character Arc)

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON:
{{
    "Ø§Ù„Ø´Ø®ØµÙŠØ©": "{char}",
    "Ø§Ù„Ø³Ù…Ø§Øª_Ø§Ù„Ø£ÙˆÙ„ÙŠØ©": [...],
    "Ù†Ù‚Ø§Ø·_Ø§Ù„ØªØ­ÙˆÙ„": [...],
    "Ø§Ù„ØªØ·ÙˆØ±_Ø§Ù„Ù†ÙØ³ÙŠ": "...",
    "Ø§Ù„Ù‚ÙˆØ³_Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠ": "...",
    "Ø§Ù„Ø¯ÙˆØ±_ÙÙŠ_Ø§Ù„Ù‚ØµØ©": "..."
}}"""
            
            result = self._call_gemini(prompt)
            try:
                json_match = re.search(r'\{[\s\S]*\}', result)
                if json_match:
                    analyses[char] = json.loads(json_match.group())
            except:
                analyses[char] = {"raw_analysis": result}
            
            time.sleep(1)  # ØªØ¬Ù†Ø¨ rate limiting
        
        return analyses
    
    def analyze_plot(self, scenes: List[Scene]) -> dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¨ÙƒØ© Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ©"""
        if not self.client:
            return {}
        
        logger.info("ğŸ“– Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¨ÙƒØ© Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© Gemini...")
        
        # ØªØ¬Ù…ÙŠØ¹ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯
        scene_summaries = []
        for scene in scenes:
            summary = {
                "Ø±Ù‚Ù…": scene.scene_number,
                "Ø§Ù„Ù…ÙƒØ§Ù†": scene.location,
                "Ø§Ù„ÙˆÙ‚Øª": scene.time_of_day,
                "Ø§Ù„Ø´Ø®ØµÙŠØ§Øª": scene.characters[:5],
                "Ø¹Ø¯Ø¯_Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª": len(scene.dialogue),
                "Ù†Øµ_Ù…Ø®ØªØµØ±": scene.full_text[:300] if scene.full_text else ""
            }
            scene_summaries.append(summary)
        
        prompt = f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ù…Ø­ØªØ±Ù ÙˆÙ…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ©.

Ø­Ù„Ù„ Ø§Ù„Ø­Ø¨ÙƒØ© Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ© Ù„Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„ØªØ§Ù„ÙŠ:

Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯: {len(scenes)}
Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯:
{json.dumps(scene_summaries[:15], ensure_ascii=False, indent=2)}

Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹ ÙŠØªØ¶Ù…Ù†:

1. **Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ©** (Three-Act Structure):
   - Ø§Ù„ØªÙ…Ù‡ÙŠØ¯ (Setup)
   - Ø§Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© (Confrontation)  
   - Ø§Ù„Ø­Ù„ (Resolution)

2. **Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø­Ø¨ÙƒØ©**:
   - Ø§Ù„ØµØ±Ø§Ø¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
   - Ø§Ù„ØµØ±Ø§Ø¹Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©
   - Ù†Ù‚Ø·Ø© Ø§Ù„ØªØ­ÙˆÙ„ Ø§Ù„Ø£ÙˆÙ„Ù‰
   - Ø§Ù„Ø°Ø±ÙˆØ© (Climax)
   - Ø§Ù„Ø­Ù„

3. **Ø§Ù„Ø«ÙŠÙ…Ø§Øª ÙˆØ§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª** Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

4. **Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹ Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠ** (Pacing)

5. **Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ø¶Ø¹Ù** ÙÙŠ Ø§Ù„Ø­Ø¨ÙƒØ©

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON:
{{
    "Ø§Ù„Ø¨Ù†ÙŠØ©_Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ©": {{
        "Ø§Ù„ØªÙ…Ù‡ÙŠØ¯": "...",
        "Ø§Ù„Ù…ÙˆØ§Ø¬Ù‡Ø©": "...",
        "Ø§Ù„Ø­Ù„": "..."
    }},
    "Ø§Ù„ØµØ±Ø§Ø¹_Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "...",
    "Ø§Ù„ØµØ±Ø§Ø¹Ø§Øª_Ø§Ù„ÙØ±Ø¹ÙŠØ©": [...],
    "Ù†Ù‚Ø§Ø·_Ø§Ù„ØªØ­ÙˆÙ„": [...],
    "Ø§Ù„Ø°Ø±ÙˆØ©": "...",
    "Ø§Ù„Ø«ÙŠÙ…Ø§Øª": [...],
    "Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹": "...",
    "Ù†Ù‚Ø§Ø·_Ø§Ù„Ù‚ÙˆØ©": [...],
    "Ù†Ù‚Ø§Ø·_Ø§Ù„Ø¶Ø¹Ù": [...],
    "Ø§Ù„ØªÙ‚ÙŠÙŠÙ…_Ø§Ù„Ø¹Ø§Ù…": "..."
}}"""
        
        result = self._call_gemini(prompt)
        try:
            json_match = re.search(r'\{[\s\S]*\}', result)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        return {"raw_analysis": result}
    
    def generate_screenplay_report(self, scenes: List[Scene], output_dir: Path) -> dict:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ø¹Ù† Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ"""
        if not self.client:
            logger.warning("Gemini ØºÙŠØ± Ù…ØªÙˆÙØ± - ØªØ®Ø·ÙŠ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
            return {}
        
        logger.info("ğŸ“Š ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„...")
        
        report = {
            "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª_Ø¹Ø§Ù…Ø©": {
                "Ø¹Ø¯Ø¯_Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯": len(scenes),
                "Ø¹Ø¯Ø¯_Ø§Ù„Ø´Ø®ØµÙŠØ§Øª": len(set(c for s in scenes for c in s.characters)),
                "Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª": sum(len(s.dialogue) for s in scenes)
            },
            "ØªØ­Ù„ÙŠÙ„_Ø§Ù„Ù…Ø´Ø§Ø¹Ø±": self.analyze_sentiment_deep(scenes),
            "ØªØ·ÙˆØ±_Ø§Ù„Ø´Ø®ØµÙŠØ§Øª": self.analyze_character_development(scenes),
            "ØªØ­Ù„ÙŠÙ„_Ø§Ù„Ø­Ø¨ÙƒØ©": self.analyze_plot(scenes)
        }
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_path = output_dir / "gemini_analysis_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Gemini: {report_path}")
        return report

# ---------------------------------------------------------
# 8. Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØµØ¯ÙŠØ± ÙˆØ§Ù„Ø¥Ù†ØªØ§Ø¬ (Production Exporter)
# ---------------------------------------------------------
class DatasetExporter:
    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def export_contextual_alpaca(self, scenes: List[Scene]):
        """
        ØªØµØ¯ÙŠØ± Ø¨ØµÙŠØºØ© Alpaca Ù…Ø¹ Ù†Ø§ÙØ°Ø© Ø³ÙŠØ§Ù‚ (Sliding Window).
        Ù‡Ø°Ø§ ÙŠØ¬Ø¹Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠÙÙ‡Ù… ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø­ÙˆØ§Ø± Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø²ÙˆÙ„Ø©.
        """
        data = []
        for scene in scenes:
            dialogue = scene.dialogue
            if not dialogue: continue

            # Ù†Ø§ÙØ°Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ (Ù‚Ø§Ø¦Ù…Ø© Ø§Ù†ØªØ¸Ø§Ø±)
            context_buffer = []
            
            # Ø¥Ø¶Ø§ÙØ© ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ ÙƒØ£ÙˆÙ„ Ø³ÙŠØ§Ù‚
            scene_setup = f"Ø§Ù„Ù…Ø´Ù‡Ø¯: {scene.heading}\nØ§Ù„Ù…ÙƒØ§Ù†: {scene.location}\nØ§Ù„ÙˆÙ‚Øª: {scene.time_of_day}"
            
            for i, turn in enumerate(dialogue):
                # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø³ÙŠØ§Ù‚ Ø³Ø§Ø¨Ù‚ØŒ Ù†Ø³ØªØ®Ø¯Ù… ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯
                current_history = "\n".join(context_buffer) if context_buffer else "Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø±."
                
                full_input = f"{scene_setup}\n\nØ³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ø³Ø§Ø¨Ù‚:\n{current_history}\n\nØ§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø«Ø© Ø§Ù„Ø¢Ù†: {turn.speaker}"
                
                if turn.sentiment != "unknown":
                    full_input += f" (Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø´Ø¹ÙˆØ±ÙŠØ©: {turn.sentiment})"

                entry = {
                    "instruction": f"Ø£Ù†Øª ØªÙ„Ø¹Ø¨ Ø¯ÙˆØ± '{turn.speaker}'. Ø£ÙƒÙ…Ù„ Ø§Ù„Ø­ÙˆØ§Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„ÙˆØµÙ.",
                    "input": full_input,
                    "output": turn.text
                }
                data.append(entry)

                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ø§ÙØ°Ø©: Ù†Ø¶ÙŠÙ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ
                context_buffer.append(f"{turn.speaker}: {turn.text}")
                # Ù†Ø­Ø°Ù Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¥Ø°Ø§ ØªØ¬Ø§ÙˆØ²Ù†Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­
                if len(context_buffer) > Config.CONTEXT_WINDOW_SIZE:
                    context_buffer.pop(0)

        self._write_json(data, "train_alpaca_contextual.json")

    def export_sharegpt(self, scenes: List[Scene]):
        """ØªØµØ¯ÙŠØ± Ø¨ØµÙŠØºØ© ShareGPT (Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©)"""
        data = []
        for scene in scenes:
            if not scene.dialogue: continue
            
            conversations = [{
                "from": "system",
                "value": f"Ù‡Ø°Ø§ Ù…Ø´Ù‡Ø¯ ØªÙ…Ø«ÙŠÙ„ÙŠ ÙŠØ¯ÙˆØ± ÙÙŠ {scene.location} ({scene.time_of_day}). ØªÙ‚Ù…Øµ Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø¨Ø¯Ù‚Ø©."
            }]
            
            for turn in scene.dialogue:
                conversations.append({
                    "from": "user",
                    "value": f"[{turn.speaker}]: {turn.text}"
                })
            
            data.append({"conversations": conversations})
        
        self._write_json(data, "train_sharegpt.json")

    def export_rag_jsonl(self, scenes: List[Scene]):
        """ØªØµØ¯ÙŠØ± Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ø¨Ø­Ø« (RAG)"""
        data = [asdict(s) for s in scenes]
        with open(self.output_dir / "rag_dataset.jsonl", 'w', encoding='utf-8') as f:
            for entry in data:
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        logger.info("ØªÙ… ØªØµØ¯ÙŠØ± Ù…Ù„ÙØ§Øª RAG.")

    def export_stats(self, graph):
        """Export character statistics to CSV"""
        if graph is None or not NETWORKX_AVAILABLE:
            logger.warning("ØªØ®Ø·ÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª - Ø§Ù„Ø´Ø¨ÙƒØ© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")
            return
        stats = []
        for node in graph.nodes():
            stats.append({
                "character": node,
                "interactions": graph.degree(node),
                "centrality": nx.degree_centrality(graph)[node] if len(graph) > 0 else 0
            })
        stats = sorted(stats, key=lambda x: x['interactions'], reverse=True)
        pd.DataFrame(stats).to_csv(self.output_dir / "character_stats.csv", index=False)
        logger.info(f"ØªÙ… ØªØµØ¯ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ§Øª ({len(stats)} Ø´Ø®ØµÙŠØ©)")

    def export_dialogue_csv(self, scenes: List[Scene]):
        """ØªØµØ¯ÙŠØ± Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø¨ØµÙŠØºØ© CSV"""
        rows = []
        for scene in scenes:
            for turn in scene.dialogue:
                rows.append({
                    "scene_id": scene.scene_id,
                    "scene_number": scene.scene_number,
                    "location": scene.location,
                    "time_of_day": scene.time_of_day,
                    "speaker": turn.speaker,
                    "text": turn.text,
                    "normalized_text": turn.normalized_text,
                    "sentiment": turn.sentiment,
                    "sentiment_score": turn.sentiment_score,
                    "word_count": count_arabic_words(turn.text)
                })
        if rows:
            pd.DataFrame(rows).to_csv(self.output_dir / "dialogue_turns.csv", index=False, encoding='utf-8-sig')
            logger.info(f"ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª ({len(rows)} Ø¬Ù…Ù„Ø©)")

    def export_summary(self, scenes: List[Scene]):
        """ØªØµØ¯ÙŠØ± Ù…Ù„Ø®Øµ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ"""
        total_dialogue = sum(len(s.dialogue) for s in scenes)
        total_characters = len(set(c for s in scenes for c in s.characters))
        total_words = sum(count_arabic_words(s.full_text) for s in scenes)
        
        summary = {
            "Ø¹Ø¯Ø¯_Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯": len(scenes),
            "Ø¹Ø¯Ø¯_Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª": total_dialogue,
            "Ø¹Ø¯Ø¯_Ø§Ù„Ø´Ø®ØµÙŠØ§Øª": total_characters,
            "Ø¹Ø¯Ø¯_Ø§Ù„ÙƒÙ„Ù…Ø§Øª_Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ": total_words,
            "Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª_Ù„ÙƒÙ„_Ù…Ø´Ù‡Ø¯": round(total_dialogue / len(scenes), 2) if scenes else 0,
            "Ø§Ù„Ø´Ø®ØµÙŠØ§Øª": list(set(c for s in scenes for c in s.characters))
        }
        
        with open(self.output_dir / "summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        logger.info("ØªÙ… ØªØµØ¯ÙŠØ± Ù…Ù„Ø®Øµ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ")

    def _write_json(self, data: Any, filename: str):
        path = self.output_dir / filename
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"ØªÙ… ØªØµØ¯ÙŠØ±: {filename} ({len(data)} Ø¹ÙŠÙ†Ø©)")

# ---------------------------------------------------------
# 9. Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Main Orchestrator)
# ---------------------------------------------------------
def main(input_path: str, output_folder: str = "alrawi_output"):
    print(f"\n--- Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø§ÙˆÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {input_path} ---")
    start_global = time.time()
    
    # Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© API Key
    if UNSTRUCTURED_API_KEY:
        logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ UNSTRUCTURED_API_KEY Ù…Ù† Ù…Ù„Ù .env")
    else:
        logger.warning("âš ï¸ UNSTRUCTURED_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…Ù„Ù .env")

    # 1. Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© (Ingestion) - Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
    ingestor = get_ingestor(input_path)
    raw_lines = ingestor.process(input_path)
    
    if not raw_lines:
        print("ÙØ´Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ.")
        return

    # 2. Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Parsing)
    parser = ScreenplayParser()
    scenes = parser.parse(raw_lines)
    print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(scenes)} Ù…Ø´Ù‡Ø¯.")

    # 3. Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡ (Enrichment)
    social_graph = None
    if ML_AVAILABLE:
        enricher = AIEnricher(use_gpu=True) # Ø§Ø¬Ø¹Ù„Ù‡Ø§ False Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ GPU
        enricher.enrich(scenes)
        social_graph = enricher.build_social_graph(scenes)

    # 4. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¨Ù€ Gemini
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
    Path(output_folder).mkdir(parents=True, exist_ok=True)
    
    if GEMINI_AVAILABLE and GEMINI_API_KEY:
        gemini_analyzer = GeminiAnalyzer()
        gemini_analyzer.generate_screenplay_report(scenes, Path(output_folder))

    # 5. Ø§Ù„ØªØµØ¯ÙŠØ± (Export)
    exporter = DatasetExporter(output_folder)
    exporter.export_contextual_alpaca(scenes) # Alpaca Ø§Ù„Ù…Ø·ÙˆØ±
    exporter.export_sharegpt(scenes)          # ShareGPT
    exporter.export_rag_jsonl(scenes)         # Vector DB
    exporter.export_dialogue_csv(scenes)      # CSV Ù„Ù„Ø­ÙˆØ§Ø±Ø§Øª
    exporter.export_summary(scenes)           # Ù…Ù„Ø®Øµ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ
    if ML_AVAILABLE and social_graph:
        exporter.export_stats(social_graph)

    print(f"\nâœ… ØªÙ…Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ {time.time() - start_global:.2f} Ø«Ø§Ù†ÙŠØ©.")
    print(f"ğŸ“‚ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯: {output_folder}")

if __name__ == "__main__":
    # ===============================================
    # âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª - Ø¹Ø¯Ù‘Ù„ Ù‡Ù†Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©
    # ===============================================
    DEFAULT_INPUT_FILE = r"E:\PREPA\Extracted_Dataset\1.txt"
    DEFAULT_OUTPUT_DIR = "dataset_output"
    # ===============================================
    
    import argparse
    parser = argparse.ArgumentParser(description="Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
    parser.add_argument("--input", default=DEFAULT_INPUT_FILE, help="Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ")
    parser.add_argument("--out", default=DEFAULT_OUTPUT_DIR, help="Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª")
    args = parser.parse_args()
    
    main(args.input, args.out)